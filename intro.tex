The scale and elasticity of cloud computing systems have 
fueled remarkable innovation and unprecedented commercial 
investment.  Cloud users ``rent'' virtualized resources (while sharing
the underlying physical resources) on a pay-per-use basis
in exchange for availability guarantees specified via service level
agreements (SLAs). Uniquely, cloud systems can be configured 
to add and remove (i.e. auto-scale) resources and services
automatically, based on the ``elastic'' resource requirements and service needs 
of executing applications.

To date however, clouds are used more for
enterprise services (object stores, databases, application servers, etc.)
than for elastic applications.
The reason is that it is challenging to configure complex
distributed systems for application use, 
and to leverage the auto-scaling that
clouds offer. To address this challenge, cloud providers 
have started to offer programming and execution environments
that preclude the need for server configuration,
under the \textit{serverless} moniker~\cite{ref:jonas2017occupy,ref:onesteptwostep,ref:peeking}.  
Serverless platforms automatically configure, manage, and scale applications
to significantly simplify cloud use.

Using the serverless model, application developers upload arbitrary computations in high level languages as stateless functions to cloud-hosted, serverless platforms, where functions are triggered automatically by the cloud in response to updates from other cloud services (e.g. storage, queues, notification services, and API gateways, 
among others). Serverless functions must execute under a time bound (e.g. 15 minutes) and an allocated memory size 
(e.g. 3GB) or else the platform will terminate the function. They
communicate, persist, and access data only 
through their inputs or via shared storage services. As a result, serverless applications are inherently elastic and can 
implement highly concurrent and parallel tasks.
In public clouds, users pay a small fee for the resources their 
functions use during execution, resulting in very low cost cloud use.
Although now available from all public cloud providers and as open 
source for private cloud systems, 
Amazon Web Services (AWS) Lambda~\cite{ref:awslambdadg}
%\footnote{\url{https://docs.aws.amazon.com/lambda/latest/dg/lambda-dg.pdf}}
was the first and is the most widely used
serverless public cloud platform.
%awslambda: https://docs.aws.amazon.com/lambda/latest/dg/lambda-dg.pdf

In this work, we investigate the efficacy of using AWS Lambda
for tuning machine learning applications in parallel.
To date, Lambda is not widely used for training and evaluating
machine learning models because of a concern that 
doing so will result in high overhead (i.e. be costly) because
of the stateless nature of serverless functions~\cite{ref:onesteptwostep}.
At the same time, identifying the ``best'' configuration for advanced
machine learning models is challenging given the large number of configuration
options (i.e. hyperparameters) typical for models today.
Hyperparameters govern the learning process of machine learning applications.
Given that parameter sweeps are embarrassingly parallel, we believe
that such tuning is a good fit for the serverless model.
To investigate this potential, its overhead, and to simplify the 
use of Lambda for training, testing, and evaluation of machine learning models, 
we design and develop a new system and toolset called \textit{Seneca}.

Seneca implements, packages, and deploys 
machine learning applications as stateless functions to AWS Lambda.
It then orchestrates exhaustive evaluation of specified hyperparameter settings
to identify the best performing model (for a given dataset) by
comparing error and accuracy across models.  We consider
prediction accuracy (as opposed to explanatory power) as the
scoring metric (mean squared error for regression 
and accuracy percentage for classification), to avoid overfitting.
Users present Seneca with their application, a range of values for 
each hyperparameter (or the default can be used), and a representative dataset.
Seneca produces, tests, and evaluates models for all combinations 
of hyperparameters and returns to the user
the set of parameters (or the model itself) that produces
the best cross-validation score.
Users can employ this model for other datasets
(with Seneca if desired) without retraining the model to amortize the cost of Seneca further.

We deploy Seneca on AWS Lambda and evaluate its tuning performance, cost,
and memory use for five machine learning applications and datasets. We 
find that Seneca is fast, inexpensive, and effective for model 
construction and comparison. Seneca is also able to identify
automatically the
best memory configuration for each application, further lowering its cost
by 10-35\%. Relative to execution in AWS EC2, Seneca enables average speedups of 294x for each additional dollar 
spent for the applications and datasets we consider.
We intend to make Seneca, its applications, and their datasets publicly available
when/if this paper is accepted for publication.  
We next overview our design and implementation of Seneca and then
present our empirical methodology and results.